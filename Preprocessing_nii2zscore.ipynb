{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NiftiファイルからZ-scoreを自動抽出\n",
    "---\n",
    "\n",
    "引数：4D化した.niiファイルがあるフォルダまでのパス / 書き出すcsvの保存先 / mask名\n",
    "\n",
    "---\n",
    "\n",
    "入力：4D化した.niiファイルがあるフォルダまでのパス / 書き出すcsvの保存先 / mask名（chunks_list.csv, targets_list.csv）\n",
    "\n",
    "---\n",
    "\n",
    "出力：all_raw.csv / raw_al45.csv / raw_al135.csv（chunks_list.csv, targets_list.csv）\n",
    "\n",
    "---\n",
    "\n",
    "注意：このプログラムはUbuntuでやったほうがいいかも（抽出する部位が多いほどデータが重くなってメモリがたりなくなる！）\n",
    "\n",
    "\n",
    "注意：このプログラムを実行する前に以下の作業をすること\n",
    "\n",
    "3D脳画像データ（前処理済みの.niiファイル）を4Dに変換\n",
    "\n",
    "1. SPM起動\n",
    "2. Batchを開く\n",
    "3. 画面左上SPM → Util → 3D to 4D File Conversion\n",
    "3. 3D Volumesから4Dに変換したいファイル('swr'のついたniftiファイル全て)を選択\n",
    "4. Output Filenameでファイル名を指定\n",
    "5. 再生ボタンを押す\n",
    "\n",
    "Motor mask（Z-score化したいボクセルの位置）の作成\n",
    "※ wfu_pickatlasはmatlabサーバーで実行（Macだと.nii形式で保存できなかった）\n",
    "※ 中山メモ：ubuntuでshareで作業するにはmatlabまで移動してshareを開く，基本的にsudoを使って作業（matlabもsudoで起動）\n",
    "\n",
    "1. wfu_pickatlasフォルダをダウンロード（ネットから or Shareかnakayamaフォルダのどっかにある）\n",
    "2. MATLABでwfu_pickatlasフォルダのパスを通す\n",
    "3. MATLABコマンドライン上で > wfu_pickatlas で起動\n",
    "4. 画面左からマスクしたい部分を選択（全脳の場合は TD Hemispheres を全選択）\n",
    "5. SAVE MASKからマスク画像をmaskとして保存（名前に'-'とか入れない）\n",
    "6. SPM → fMRI起動\n",
    "7. MenuからNormalize(Est & Wri)を選択\n",
    "8. Bath Editorが立ち上がったらDataをダブルクリック\n",
    "9. Image to Alignには'swr'がファイル名についているniftiファイルを適当に1つ選択\n",
    "10. Image to Writeには先ほど作成したmaskファイルを選択\n",
    "11. 再生ボタンを押す → wmask.niiができる\n",
    "12. Batch Editorに戻りRealign(Reslice)を選択\n",
    "13. Imagesをダブルクリック\n",
    "14. 'swr'がファイル名についているniftiファイルを適当に一つ，先ほどできたwmask.niiファイルの順で選択\n",
    "15. 再生ボタンを押す → rwmask.niiができる\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mvpa2.suite import *\n",
    "from mvpa2.datasets.mri import fmri_dataset\n",
    "import os\n",
    "import os.path\n",
    "from os.path import join as pathjoin\n",
    "from pprint import pprint\n",
    "# from nifti import NiftiImage\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "# import dill\n",
    "import csv\n",
    "\n",
    "import nibabel as nib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# コマンドライン引数で.niiファイルがあるディレクトリまでのパスを取得．\n",
    "# !!!!!!!!!! scan_numらへん，dataset，zscoreのパラメータ書き換え\n",
    "\n",
    "# args = sys.argv\n",
    "# PATH = args[1]\n",
    "# PATH_save = args[2]\n",
    "# MASK_name = args[3]\n",
    "\n",
    "# jupyter notebookのときはここで指定\n",
    "PATH = '../../Data_mri/angledlinevid-1fe/20181119tm/'\n",
    "PATH_save = '../MaskBrodmann/20181227rn/'\n",
    "MASK_name = 'rwmaskBA.nii'\n",
    "\n",
    "PATH_mask = PATH + MASK_name\n",
    "\n",
    "# 前処理済みならswrがついた.niiファイルを選択\n",
    "PATH_nii = PATH + '4D.nii'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RawDataのディレクトリ名・パス\n",
    "DIR_RAW = PATH_save + 'RawData'\n",
    "PATH_RAW = DIR_RAW + '/'\n",
    "\n",
    "# すでに存在する場合は何もせず，存在していない場合はディレクトリ作成\n",
    "if not os.path.exists(DIR_RAW):\n",
    "    os.mkdir(DIR_RAW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headcoil = PATH.split('/')[5]\n",
    "\n",
    "\n",
    "# 総スキャン数\n",
    "scan_num = 200\n",
    "\n",
    "# restのスキャン数\n",
    "restNum = 8\n",
    "\n",
    "# 1タスクのスキャン数\n",
    "taskNum = 88\n",
    "\n",
    "# 試行数\n",
    "runNum = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitTaks関数\n",
    "\n",
    "全てのZ-scoreをまとめたものdataで受け取り，Rock時とscissor時とpaper時のデータを分けてcsvファイルで書き出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitTask(brain):\n",
    "\n",
    "    # 各タスクごとのマスク作成\n",
    "    maskAl45 = ([False] * restNum) + ([False] * taskNum) + ([False] * restNum) + ([True] * taskNum) + ([False] * restNum) + ([False] * restNum) + ([True] * taskNum) + ([False] * restNum) + ([False] * taskNum) + ([False] * restNum) + ([False] * restNum) + ([False] * taskNum) + ([False] * restNum) + ([True] * taskNum) + ([False] * restNum) + ([False] * restNum) + ([False] * taskNum) + ([False] * restNum) + ([True] * taskNum) + ([False] * restNum)\n",
    "\n",
    "    maskAl135 = ([False] * restNum) + ([True] * taskNum) + ([False] * restNum) + ([False] * taskNum) + ([False] * restNum) + ([False] * restNum) + ([False] * taskNum) + ([False] * restNum) + ([True] * taskNum) + ([False] * restNum) + ([False] * restNum) + ([True] * taskNum) + ([False] * restNum) + ([False] * taskNum) + ([False] * restNum) + ([False] * restNum) + ([True] * taskNum) + ([False] * restNum) + ([False] * taskNum) + ([False] * restNum)\n",
    "\n",
    "    # mask適用\n",
    "    dataAl45 = brain[maskAl45]\n",
    "\n",
    "    dataAl135 = brain[maskAl135]\n",
    "\n",
    "    # splitVoxRun関数で各ボクセル，各試行で分割，csv書き出し\n",
    "    vox_run_al45 = splitVoxRun(dataAl45)\n",
    "    al45_file = PATH_RAW + 'raw_al45.csv'\n",
    "    vox_run_al45.to_csv(al45_file)\n",
    "    print(al45_file)\n",
    "\n",
    "    vox_run_al135 = splitVoxRun(dataAl135)\n",
    "    al135_file = PATH_RAW + 'raw_al135.csv'\n",
    "    vox_run_al135.to_csv(al135_file)\n",
    "    print(al135_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitVoxRun関数\n",
    "\n",
    "引数に全ボクセルのデータをまとめたデータフレームを受け取り，各ボクセルで試行ごとに分割結合する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitVoxRun(data):\n",
    "\n",
    "    # 各試行ごとに分割，横結合（Voxel1-Run1, Voxel1-Run2, Voxel1-Run3, Voxel1-Run4, Voxel2-Run1, Voxel2-Run2...）\n",
    "\n",
    "    # データ格納用\n",
    "    vox_run_all = pd.DataFrame(index = [], columns = [])\n",
    "\n",
    "    for i in range(len(data.columns)):\n",
    "\n",
    "        print('----- Voxel' + str(i + 1) + ' -----')\n",
    "\n",
    "        # ボクセルで試行ごとに分割，reshapeを使って1列データを(試行数，1タスクのスキャン数)に\n",
    "        vox_run = np.reshape(list(data.iloc[:, i]), (runNum, taskNum))\n",
    "\n",
    "        # 転置してデータフレーム化\n",
    "        vox_run = pd.DataFrame(vox_run).T\n",
    "\n",
    "        # 列名つける\n",
    "        col_name = ['Voxel' + str(i + 1)] * runNum\n",
    "        vox_run.columns = col_name\n",
    "\n",
    "        # データ格納\n",
    "        vox_run_all = pd.concat([vox_run_all, vox_run], axis = 1)\n",
    "\n",
    "    return vox_run_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main関数\n",
    "\n",
    "* fMRIデータ（niftiファイル）読み込み\n",
    "* 全ボクセルデータZ-score化，抽出，書きだし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # 4D化した.niiファイル名リストを作成\n",
    "\n",
    "    nifti = [PATH_nii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # 教師データの作成（この作業なしにやる方法がわからないので，必要のない作業ではあるがやる）\n",
    "\n",
    "    task_p1 = ['-1'] * restNum\n",
    "    task_p2 = ['-1'] * restNum\n",
    "\n",
    "    task45 = ['0'] * taskNum\n",
    "    task135 = ['1'] * taskNum\n",
    "    taskRest = ['-1'] * restNum\n",
    "\n",
    "    # pattern1:Rest -> 45 -> Rest -> 135 -> Rest\n",
    "    task_p1.extend(task45)\n",
    "    task_p1.extend(taskRest)\n",
    "    task_p1.extend(task135)\n",
    "    task_p1.extend(taskRest)\n",
    "\n",
    "    # pattern2:Rest -> 135 -> Rest -> 45 -> Rest\n",
    "    task_p2.extend(task135)\n",
    "    task_p2.extend(taskRest)\n",
    "    task_p2.extend(task45)\n",
    "    task_p2.extend(taskRest)\n",
    "\n",
    "    task = task_p2 + task_p1 + task_p2 + task_p2\n",
    "\n",
    "    target = pd.DataFrame(task)\n",
    "\n",
    "    PATH_target = PATH + 'targets_list.csv'\n",
    "    target.to_csv(PATH_target, index = False, header = None)\n",
    "    print('target')\n",
    "\n",
    "    targets_list = []\n",
    "\n",
    "    targets_file = open(PATH_target, 'rU')\n",
    "    dataReader = csv.reader(targets_file)\n",
    "\n",
    "    for row in dataReader:\n",
    "        targets_list.append(row[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    # チャンク（試行数リスト？）の作成（この作業なしにやる方法がわからないので，必要のない作業ではあるがやる）\n",
    "\n",
    "    chunk = ['1'] * scan_num\n",
    "    chunk0 = ['2'] * scan_num\n",
    "    chunk1 = ['3'] * scan_num\n",
    "    chunk2 = ['4'] * scan_num\n",
    "\n",
    "    chunk.extend(chunk0)\n",
    "    chunk.extend(chunk1)\n",
    "    chunk.extend(chunk2)\n",
    "\n",
    "    chunks = pd.DataFrame(chunk)\n",
    "\n",
    "    PATH_chunk = PATH + 'chunks_list.csv'\n",
    "    chunks.to_csv(PATH_chunk, index = False, header = None)\n",
    "    print('chunks')\n",
    "\n",
    "    chunks_list = []\n",
    "\n",
    "    chunks = open(PATH_chunk, 'rU')\n",
    "\n",
    "    for x in chunks:\n",
    "        chunks_list.append(x.rstrip('\\r\\n'))\n",
    "\n",
    "    chunks.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    # データセットの整形\n",
    "\n",
    "    dataset = fmri_dataset(nifti, targets=targets_list, chunks=chunks_list, mask=PATH_mask ,sprefix='voxel', tprefix='time', add_fa=None)\n",
    "\n",
    "    print('dataset ready')\n",
    "\n",
    "    poly_detrend(dataset, polyord=1, chunks_attr='chunks')\n",
    "\n",
    "    dataset = dataset[np.array([l in ['-1', '0', '1']\n",
    "                               for l in dataset.targets], dtype='bool')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Z-score抽出 → データフレーム化\n",
    "    zscore(dataset, chunks_attr='chunks', param_est=('targets', ['0', '1']), dtype='float32')\n",
    "    print('z-score')\n",
    "\n",
    "    VoxelData = pd.DataFrame(np.array(dataset[:,:]))\n",
    "    print(VoxelData .shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    voxName = []\n",
    "\n",
    "    for i in range(len(VoxelData.columns)):\n",
    "\n",
    "        name = 'Voxel' + str(i+1)\n",
    "        print(name)\n",
    "\n",
    "        voxName.append(name)\n",
    "\n",
    "    VoxelData.columns = voxName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # 全データのcevファイル書き出し\n",
    "    # PATH_all = PATH_RAW + 'all_raw.csv'\n",
    "    # VoxelData.to_csv(PATH_all, index = False)\n",
    "    # print(PATH_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    splitTask(VoxelData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # もしデータセットの整形でサイズエラーが出た時は以下で.niiファイル読み込みしてサイズを確認する\n",
    "    # nii0 = nib.load('../../Data_mri/tappingState-2fe/20181029rn/64ch/wmaskAll.nii')\n",
    "    # img0 = nii0.get_data()\n",
    "    # img0.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
